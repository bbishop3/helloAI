{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbishop3/helloAI/blob/main/colab/metaltest1_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CzeLeqEaNj8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5589c4-eb19-4f4e-8195-7fe575e6b161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 16452066.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 303193.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5541611.33it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 13038814.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "starting timer for training using CPU...\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.312174  [   64/60000]\n",
            "loss: 2.301783  [ 6464/60000]\n",
            "loss: 2.277236  [12864/60000]\n",
            "loss: 2.263490  [19264/60000]\n",
            "loss: 2.259455  [25664/60000]\n",
            "loss: 2.215531  [32064/60000]\n",
            "loss: 2.229321  [38464/60000]\n",
            "loss: 2.194198  [44864/60000]\n",
            "loss: 2.190401  [51264/60000]\n",
            "loss: 2.161554  [57664/60000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.174170  [   64/60000]\n",
            "loss: 2.169331  [ 6464/60000]\n",
            "loss: 2.110966  [12864/60000]\n",
            "loss: 2.118908  [19264/60000]\n",
            "loss: 2.083627  [25664/60000]\n",
            "loss: 2.018769  [32064/60000]\n",
            "loss: 2.043723  [38464/60000]\n",
            "loss: 1.974871  [44864/60000]\n",
            "loss: 1.966372  [51264/60000]\n",
            "loss: 1.903933  [57664/60000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.941159  [   64/60000]\n",
            "loss: 1.919578  [ 6464/60000]\n",
            "loss: 1.806982  [12864/60000]\n",
            "loss: 1.831196  [19264/60000]\n",
            "loss: 1.733498  [25664/60000]\n",
            "loss: 1.684940  [32064/60000]\n",
            "loss: 1.692337  [38464/60000]\n",
            "loss: 1.606697  [44864/60000]\n",
            "loss: 1.613496  [51264/60000]\n",
            "loss: 1.508690  [57664/60000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.604376  [   64/60000]\n",
            "loss: 1.574332  [ 6464/60000]\n",
            "loss: 1.425849  [12864/60000]\n",
            "loss: 1.484146  [19264/60000]\n",
            "loss: 1.359568  [25664/60000]\n",
            "loss: 1.359564  [32064/60000]\n",
            "loss: 1.366227  [38464/60000]\n",
            "loss: 1.299685  [44864/60000]\n",
            "loss: 1.325425  [51264/60000]\n",
            "loss: 1.224134  [57664/60000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.339895  [   64/60000]\n",
            "loss: 1.324828  [ 6464/60000]\n",
            "loss: 1.160364  [12864/60000]\n",
            "loss: 1.257918  [19264/60000]\n",
            "loss: 1.119740  [25664/60000]\n",
            "loss: 1.151840  [32064/60000]\n",
            "loss: 1.172978  [38464/60000]\n",
            "loss: 1.114732  [44864/60000]\n",
            "loss: 1.146323  [51264/60000]\n",
            "loss: 1.065000  [57664/60000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.166179  [   64/60000]\n",
            "loss: 1.171938  [ 6464/60000]\n",
            "loss: 0.989205  [12864/60000]\n",
            "loss: 1.120196  [19264/60000]\n",
            "loss: 0.973036  [25664/60000]\n",
            "loss: 1.015151  [32064/60000]\n",
            "loss: 1.056792  [38464/60000]\n",
            "loss: 1.000258  [44864/60000]\n",
            "loss: 1.031415  [51264/60000]\n",
            "loss: 0.967346  [57664/60000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.048016  [   64/60000]\n",
            "loss: 1.074878  [ 6464/60000]\n",
            "loss: 0.873981  [12864/60000]\n",
            "loss: 1.029329  [19264/60000]\n",
            "loss: 0.880962  [25664/60000]\n",
            "loss: 0.920313  [32064/60000]\n",
            "loss: 0.981513  [38464/60000]\n",
            "loss: 0.927072  [44864/60000]\n",
            "loss: 0.953087  [51264/60000]\n",
            "loss: 0.902205  [57664/60000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.962363  [   64/60000]\n",
            "loss: 1.008018  [ 6464/60000]\n",
            "loss: 0.792527  [12864/60000]\n",
            "loss: 0.965011  [19264/60000]\n",
            "loss: 0.820022  [25664/60000]\n",
            "loss: 0.851241  [32064/60000]\n",
            "loss: 0.928842  [38464/60000]\n",
            "loss: 0.878723  [44864/60000]\n",
            "loss: 0.897056  [51264/60000]\n",
            "loss: 0.854970  [57664/60000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.897068  [   64/60000]\n",
            "loss: 0.957721  [ 6464/60000]\n",
            "loss: 0.731880  [12864/60000]\n",
            "loss: 0.916667  [19264/60000]\n",
            "loss: 0.777234  [25664/60000]\n",
            "loss: 0.799470  [32064/60000]\n",
            "loss: 0.889080  [38464/60000]\n",
            "loss: 0.845339  [44864/60000]\n",
            "loss: 0.855238  [51264/60000]\n",
            "loss: 0.818524  [57664/60000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.845039  [   64/60000]\n",
            "loss: 0.917344  [ 6464/60000]\n",
            "loss: 0.684911  [12864/60000]\n",
            "loss: 0.878826  [19264/60000]\n",
            "loss: 0.745529  [25664/60000]\n",
            "loss: 0.759951  [32064/60000]\n",
            "loss: 0.857000  [38464/60000]\n",
            "loss: 0.821080  [44864/60000]\n",
            "loss: 0.822775  [51264/60000]\n",
            "loss: 0.788862  [57664/60000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.802113  [   64/60000]\n",
            "loss: 0.883083  [ 6464/60000]\n",
            "loss: 0.647208  [12864/60000]\n",
            "loss: 0.848293  [19264/60000]\n",
            "loss: 0.720867  [25664/60000]\n",
            "loss: 0.729022  [32064/60000]\n",
            "loss: 0.829391  [38464/60000]\n",
            "loss: 0.802138  [44864/60000]\n",
            "loss: 0.796606  [51264/60000]\n",
            "loss: 0.763723  [57664/60000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.765273  [   64/60000]\n",
            "loss: 0.852863  [ 6464/60000]\n",
            "loss: 0.615787  [12864/60000]\n",
            "loss: 0.823069  [19264/60000]\n",
            "loss: 0.700637  [25664/60000]\n",
            "loss: 0.704205  [32064/60000]\n",
            "loss: 0.804785  [38464/60000]\n",
            "loss: 0.786327  [44864/60000]\n",
            "loss: 0.774675  [51264/60000]\n",
            "loss: 0.741650  [57664/60000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.732837  [   64/60000]\n",
            "loss: 0.825641  [ 6464/60000]\n",
            "loss: 0.589064  [12864/60000]\n",
            "loss: 0.801557  [19264/60000]\n",
            "loss: 0.683489  [25664/60000]\n",
            "loss: 0.683920  [32064/60000]\n",
            "loss: 0.782263  [38464/60000]\n",
            "loss: 0.772463  [44864/60000]\n",
            "loss: 0.755864  [51264/60000]\n",
            "loss: 0.721823  [57664/60000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.703856  [   64/60000]\n",
            "loss: 0.800713  [ 6464/60000]\n",
            "loss: 0.565763  [12864/60000]\n",
            "loss: 0.782900  [19264/60000]\n",
            "loss: 0.668649  [25664/60000]\n",
            "loss: 0.666842  [32064/60000]\n",
            "loss: 0.761269  [38464/60000]\n",
            "loss: 0.759861  [44864/60000]\n",
            "loss: 0.739325  [51264/60000]\n",
            "loss: 0.703666  [57664/60000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.677681  [   64/60000]\n",
            "loss: 0.777651  [ 6464/60000]\n",
            "loss: 0.545054  [12864/60000]\n",
            "loss: 0.766294  [19264/60000]\n",
            "loss: 0.655652  [25664/60000]\n",
            "loss: 0.652162  [32064/60000]\n",
            "loss: 0.741604  [38464/60000]\n",
            "loss: 0.748211  [44864/60000]\n",
            "loss: 0.724577  [51264/60000]\n",
            "loss: 0.686877  [57664/60000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.653809  [   64/60000]\n",
            "loss: 0.756320  [ 6464/60000]\n",
            "loss: 0.526566  [12864/60000]\n",
            "loss: 0.751286  [19264/60000]\n",
            "loss: 0.644026  [25664/60000]\n",
            "loss: 0.639361  [32064/60000]\n",
            "loss: 0.723074  [38464/60000]\n",
            "loss: 0.737388  [44864/60000]\n",
            "loss: 0.711350  [51264/60000]\n",
            "loss: 0.671190  [57664/60000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.631906  [   64/60000]\n",
            "loss: 0.736599  [ 6464/60000]\n",
            "loss: 0.509837  [12864/60000]\n",
            "loss: 0.737544  [19264/60000]\n",
            "loss: 0.633622  [25664/60000]\n",
            "loss: 0.628033  [32064/60000]\n",
            "loss: 0.705562  [38464/60000]\n",
            "loss: 0.727395  [44864/60000]\n",
            "loss: 0.699442  [51264/60000]\n",
            "loss: 0.656551  [57664/60000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.611770  [   64/60000]\n",
            "loss: 0.718287  [ 6464/60000]\n",
            "loss: 0.494606  [12864/60000]\n",
            "loss: 0.724773  [19264/60000]\n",
            "loss: 0.624297  [25664/60000]\n",
            "loss: 0.617876  [32064/60000]\n",
            "loss: 0.688990  [38464/60000]\n",
            "loss: 0.718330  [44864/60000]\n",
            "loss: 0.688868  [51264/60000]\n",
            "loss: 0.642898  [57664/60000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.593226  [   64/60000]\n",
            "loss: 0.701385  [ 6464/60000]\n",
            "loss: 0.480691  [12864/60000]\n",
            "loss: 0.712780  [19264/60000]\n",
            "loss: 0.615995  [25664/60000]\n",
            "loss: 0.608733  [32064/60000]\n",
            "loss: 0.673316  [38464/60000]\n",
            "loss: 0.710138  [44864/60000]\n",
            "loss: 0.679503  [51264/60000]\n",
            "loss: 0.630134  [57664/60000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.576155  [   64/60000]\n",
            "loss: 0.685744  [ 6464/60000]\n",
            "loss: 0.467968  [12864/60000]\n",
            "loss: 0.701577  [19264/60000]\n",
            "loss: 0.608449  [25664/60000]\n",
            "loss: 0.600484  [32064/60000]\n",
            "loss: 0.658669  [38464/60000]\n",
            "loss: 0.702955  [44864/60000]\n",
            "loss: 0.671378  [51264/60000]\n",
            "loss: 0.618202  [57664/60000]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.560440  [   64/60000]\n",
            "loss: 0.671294  [ 6464/60000]\n",
            "loss: 0.456315  [12864/60000]\n",
            "loss: 0.691046  [19264/60000]\n",
            "loss: 0.601511  [25664/60000]\n",
            "loss: 0.593066  [32064/60000]\n",
            "loss: 0.645027  [38464/60000]\n",
            "loss: 0.696781  [44864/60000]\n",
            "loss: 0.664443  [51264/60000]\n",
            "loss: 0.606922  [57664/60000]\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.546007  [   64/60000]\n",
            "loss: 0.657994  [ 6464/60000]\n",
            "loss: 0.445582  [12864/60000]\n",
            "loss: 0.681095  [19264/60000]\n",
            "loss: 0.595014  [25664/60000]\n",
            "loss: 0.586230  [32064/60000]\n",
            "loss: 0.632282  [38464/60000]\n",
            "loss: 0.691556  [44864/60000]\n",
            "loss: 0.658523  [51264/60000]\n",
            "loss: 0.596245  [57664/60000]\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.532670  [   64/60000]\n",
            "loss: 0.645736  [ 6464/60000]\n",
            "loss: 0.435689  [12864/60000]\n",
            "loss: 0.671634  [19264/60000]\n",
            "loss: 0.588899  [25664/60000]\n",
            "loss: 0.579934  [32064/60000]\n",
            "loss: 0.620463  [38464/60000]\n",
            "loss: 0.687267  [44864/60000]\n",
            "loss: 0.653478  [51264/60000]\n",
            "loss: 0.586147  [57664/60000]\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.520333  [   64/60000]\n",
            "loss: 0.634392  [ 6464/60000]\n",
            "loss: 0.426507  [12864/60000]\n",
            "loss: 0.662654  [19264/60000]\n",
            "loss: 0.583096  [25664/60000]\n",
            "loss: 0.574022  [32064/60000]\n",
            "loss: 0.609533  [38464/60000]\n",
            "loss: 0.683841  [44864/60000]\n",
            "loss: 0.649143  [51264/60000]\n",
            "loss: 0.576555  [57664/60000]\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.508867  [   64/60000]\n",
            "loss: 0.623878  [ 6464/60000]\n",
            "loss: 0.417976  [12864/60000]\n",
            "loss: 0.654103  [19264/60000]\n",
            "loss: 0.577557  [25664/60000]\n",
            "loss: 0.568419  [32064/60000]\n",
            "loss: 0.599413  [38464/60000]\n",
            "loss: 0.681083  [44864/60000]\n",
            "loss: 0.645421  [51264/60000]\n",
            "loss: 0.567388  [57664/60000]\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.498172  [   64/60000]\n",
            "loss: 0.614141  [ 6464/60000]\n",
            "loss: 0.410043  [12864/60000]\n",
            "loss: 0.645988  [19264/60000]\n",
            "loss: 0.572132  [25664/60000]\n",
            "loss: 0.563085  [32064/60000]\n",
            "loss: 0.590068  [38464/60000]\n",
            "loss: 0.678931  [44864/60000]\n",
            "loss: 0.642249  [51264/60000]\n",
            "loss: 0.558579  [57664/60000]\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.488157  [   64/60000]\n",
            "loss: 0.605139  [ 6464/60000]\n",
            "loss: 0.402706  [12864/60000]\n",
            "loss: 0.638263  [19264/60000]\n",
            "loss: 0.566858  [25664/60000]\n",
            "loss: 0.557971  [32064/60000]\n",
            "loss: 0.581471  [38464/60000]\n",
            "loss: 0.677372  [44864/60000]\n",
            "loss: 0.639510  [51264/60000]\n",
            "loss: 0.550065  [57664/60000]\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.478786  [   64/60000]\n",
            "loss: 0.596796  [ 6464/60000]\n",
            "loss: 0.395907  [12864/60000]\n",
            "loss: 0.630919  [19264/60000]\n",
            "loss: 0.561643  [25664/60000]\n",
            "loss: 0.552987  [32064/60000]\n",
            "loss: 0.573581  [38464/60000]\n",
            "loss: 0.676343  [44864/60000]\n",
            "loss: 0.637078  [51264/60000]\n",
            "loss: 0.541868  [57664/60000]\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.469971  [   64/60000]\n",
            "loss: 0.589071  [ 6464/60000]\n",
            "loss: 0.389516  [12864/60000]\n",
            "loss: 0.623933  [19264/60000]\n",
            "loss: 0.556487  [25664/60000]\n",
            "loss: 0.548114  [32064/60000]\n",
            "loss: 0.566297  [38464/60000]\n",
            "loss: 0.675697  [44864/60000]\n",
            "loss: 0.634936  [51264/60000]\n",
            "loss: 0.533949  [57664/60000]\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.461667  [   64/60000]\n",
            "loss: 0.581904  [ 6464/60000]\n",
            "loss: 0.383539  [12864/60000]\n",
            "loss: 0.617284  [19264/60000]\n",
            "loss: 0.551388  [25664/60000]\n",
            "loss: 0.543321  [32064/60000]\n",
            "loss: 0.559509  [38464/60000]\n",
            "loss: 0.675357  [44864/60000]\n",
            "loss: 0.633041  [51264/60000]\n",
            "loss: 0.526307  [57664/60000]\n",
            "completed training in ... 473.759450674057s\n",
            "starting timer for testing using CPU...\n",
            "Test Error: \n",
            " Accuracy: 80.8%, Avg loss: 0.551552 \n",
            "\n",
            "completed testing in ... 1.8664851188659668s\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Define linear model we will use below\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# modified from https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
        "# get our competing devices ready ... go ahead and init all three here, but ONLY USE ONE during each test\n",
        "# scroll down below and replace the references to gpu_, tpu_, cpu with whichever device you are testing\n",
        "# make sure you replace ALL of them\n",
        "gpu_device = torch.device(\"cuda\")\n",
        "tpu_device = torch.device(\"xla\")\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# first hyperparam\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# Show some sample data\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break\n",
        "\n",
        "\n",
        "# sample code for working with m1\n",
        "  # Create a Tensor directly on the mps device\n",
        "  #x = torch.ones(5, device=mps_device)\n",
        "  # Or\n",
        "  #x = torch.ones(5, device=\"mps\")\n",
        "  # Any operation happens on the GPU\n",
        "  #y = x * 2\n",
        "\n",
        "  # Move your model to mps just like any other device\n",
        "model = NeuralNetwork().to(cpu_device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "print(\"starting timer for training using CPU...\")\n",
        "start = time.time()\n",
        "epochs = 30 \n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer, cpu_device)\n",
        "print(f\"completed training in ... {time.time()-start}s\")\n",
        "\n",
        "print(\"starting timer for testing using CPU...\")\n",
        "start = time.time()\n",
        "test(test_dataloader, model, loss_fn, cpu_device)\n",
        "print(f\"completed testing in ... {time.time()-start}s\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Define linear model we will use below\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# modified from https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
        "# get our competing devices ready ... go ahead and init all three here, but ONLY USE ONE during each test\n",
        "# scroll down below and replace the references to gpu_, tpu_, cpu with whichever device you are testing\n",
        "# make sure you replace ALL of them\n",
        "gpu_device = torch.device(\"cuda\")\n",
        "tpu_device = torch.device(\"xla\")\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# first hyperparam\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# Show some sample data\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break\n",
        "\n",
        "\n",
        "# sample code for working with m1\n",
        "  # Create a Tensor directly on the mps device\n",
        "  #x = torch.ones(5, device=mps_device)\n",
        "  # Or\n",
        "  #x = torch.ones(5, device=\"mps\")\n",
        "  # Any operation happens on the GPU\n",
        "  #y = x * 2\n",
        "\n",
        "  # Move your model to mps just like any other device\n",
        "model = NeuralNetwork().to(cpu_device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "print(\"starting timer for training using CPU...\")\n",
        "start = time.time()\n",
        "epochs = 30 \n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer, cpu_device)\n",
        "print(f\"completed training in ... {time.time()-start}s\")\n",
        "\n",
        "print(\"starting timer for testing using CPU...\")\n",
        "start = time.time()\n",
        "test(test_dataloader, model, loss_fn, cpu_device)\n",
        "print(f\"completed testing in ... {time.time()-start}s\")"
      ],
      "metadata": {
        "id": "TO1aL4f6CyuN",
        "outputId": "c1bf34cc-fac1-4358-ced3-4e15f6587a13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:00<00:00, 146011092.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 95594503.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 4422102/4422102 [00:00<00:00, 58650518.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 21357346.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "starting timer for training using CPU...\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.304855  [   64/60000]\n",
            "loss: 2.301004  [ 6464/60000]\n",
            "loss: 2.277327  [12864/60000]\n",
            "loss: 2.276442  [19264/60000]\n",
            "loss: 2.243793  [25664/60000]\n",
            "loss: 2.214584  [32064/60000]\n",
            "loss: 2.227688  [38464/60000]\n",
            "loss: 2.186156  [44864/60000]\n",
            "loss: 2.194080  [51264/60000]\n",
            "loss: 2.156647  [57664/60000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.162164  [   64/60000]\n",
            "loss: 2.162746  [ 6464/60000]\n",
            "loss: 2.102520  [12864/60000]\n",
            "loss: 2.122948  [19264/60000]\n",
            "loss: 2.065418  [25664/60000]\n",
            "loss: 2.003555  [32064/60000]\n",
            "loss: 2.040569  [38464/60000]\n",
            "loss: 1.953958  [44864/60000]\n",
            "loss: 1.969145  [51264/60000]\n",
            "loss: 1.897242  [57664/60000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.925929  [   64/60000]\n",
            "loss: 1.904673  [ 6464/60000]\n",
            "loss: 1.785973  [12864/60000]\n",
            "loss: 1.830176  [19264/60000]\n",
            "loss: 1.713781  [25664/60000]\n",
            "loss: 1.665369  [32064/60000]\n",
            "loss: 1.693273  [38464/60000]\n",
            "loss: 1.585747  [44864/60000]\n",
            "loss: 1.616589  [51264/60000]\n",
            "loss: 1.510976  [57664/60000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.594134  [   64/60000]\n",
            "loss: 1.563101  [ 6464/60000]\n",
            "loss: 1.409554  [12864/60000]\n",
            "loss: 1.478264  [19264/60000]\n",
            "loss: 1.352446  [25664/60000]\n",
            "loss: 1.353231  [32064/60000]\n",
            "loss: 1.366432  [38464/60000]\n",
            "loss: 1.286097  [44864/60000]\n",
            "loss: 1.325673  [51264/60000]\n",
            "loss: 1.223377  [57664/60000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.331605  [   64/60000]\n",
            "loss: 1.316257  [ 6464/60000]\n",
            "loss: 1.143548  [12864/60000]\n",
            "loss: 1.244337  [19264/60000]\n",
            "loss: 1.117291  [25664/60000]\n",
            "loss: 1.147367  [32064/60000]\n",
            "loss: 1.167655  [38464/60000]\n",
            "loss: 1.100595  [44864/60000]\n",
            "loss: 1.144601  [51264/60000]\n",
            "loss: 1.060473  [57664/60000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.156097  [   64/60000]\n",
            "loss: 1.162307  [ 6464/60000]\n",
            "loss: 0.970541  [12864/60000]\n",
            "loss: 1.102449  [19264/60000]\n",
            "loss: 0.977066  [25664/60000]\n",
            "loss: 1.010702  [32064/60000]\n",
            "loss: 1.049194  [38464/60000]\n",
            "loss: 0.984598  [44864/60000]\n",
            "loss: 1.029240  [51264/60000]\n",
            "loss: 0.962595  [57664/60000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.035831  [   64/60000]\n",
            "loss: 1.064114  [ 6464/60000]\n",
            "loss: 0.854622  [12864/60000]\n",
            "loss: 1.010205  [19264/60000]\n",
            "loss: 0.891240  [25664/60000]\n",
            "loss: 0.915850  [32064/60000]\n",
            "loss: 0.974023  [38464/60000]\n",
            "loss: 0.909494  [44864/60000]\n",
            "loss: 0.951248  [51264/60000]\n",
            "loss: 0.898796  [57664/60000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.949071  [   64/60000]\n",
            "loss: 0.996591  [ 6464/60000]\n",
            "loss: 0.772786  [12864/60000]\n",
            "loss: 0.945916  [19264/60000]\n",
            "loss: 0.834392  [25664/60000]\n",
            "loss: 0.847267  [32064/60000]\n",
            "loss: 0.921929  [38464/60000]\n",
            "loss: 0.859482  [44864/60000]\n",
            "loss: 0.896170  [51264/60000]\n",
            "loss: 0.853495  [57664/60000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.883147  [   64/60000]\n",
            "loss: 0.946388  [ 6464/60000]\n",
            "loss: 0.712739  [12864/60000]\n",
            "loss: 0.898251  [19264/60000]\n",
            "loss: 0.794014  [25664/60000]\n",
            "loss: 0.796216  [32064/60000]\n",
            "loss: 0.882581  [38464/60000]\n",
            "loss: 0.824526  [44864/60000]\n",
            "loss: 0.855397  [51264/60000]\n",
            "loss: 0.818796  [57664/60000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.830699  [   64/60000]\n",
            "loss: 0.906214  [ 6464/60000]\n",
            "loss: 0.666901  [12864/60000]\n",
            "loss: 0.861631  [19264/60000]\n",
            "loss: 0.763547  [25664/60000]\n",
            "loss: 0.757188  [32064/60000]\n",
            "loss: 0.850627  [38464/60000]\n",
            "loss: 0.798557  [44864/60000]\n",
            "loss: 0.824034  [51264/60000]\n",
            "loss: 0.790720  [57664/60000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.787606  [   64/60000]\n",
            "loss: 0.872099  [ 6464/60000]\n",
            "loss: 0.630463  [12864/60000]\n",
            "loss: 0.832492  [19264/60000]\n",
            "loss: 0.739065  [25664/60000]\n",
            "loss: 0.726594  [32064/60000]\n",
            "loss: 0.823143  [38464/60000]\n",
            "loss: 0.778096  [44864/60000]\n",
            "loss: 0.798814  [51264/60000]\n",
            "loss: 0.766985  [57664/60000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.751025  [   64/60000]\n",
            "loss: 0.841923  [ 6464/60000]\n",
            "loss: 0.600430  [12864/60000]\n",
            "loss: 0.808509  [19264/60000]\n",
            "loss: 0.718433  [25664/60000]\n",
            "loss: 0.701810  [32064/60000]\n",
            "loss: 0.798445  [38464/60000]\n",
            "loss: 0.761136  [44864/60000]\n",
            "loss: 0.777659  [51264/60000]\n",
            "loss: 0.746188  [57664/60000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.719065  [   64/60000]\n",
            "loss: 0.814662  [ 6464/60000]\n",
            "loss: 0.574977  [12864/60000]\n",
            "loss: 0.788321  [19264/60000]\n",
            "loss: 0.700379  [25664/60000]\n",
            "loss: 0.681302  [32064/60000]\n",
            "loss: 0.775637  [38464/60000]\n",
            "loss: 0.746366  [44864/60000]\n",
            "loss: 0.759329  [51264/60000]\n",
            "loss: 0.727356  [57664/60000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.690743  [   64/60000]\n",
            "loss: 0.789762  [ 6464/60000]\n",
            "loss: 0.552793  [12864/60000]\n",
            "loss: 0.770643  [19264/60000]\n",
            "loss: 0.684210  [25664/60000]\n",
            "loss: 0.663861  [32064/60000]\n",
            "loss: 0.754243  [38464/60000]\n",
            "loss: 0.733169  [44864/60000]\n",
            "loss: 0.743253  [51264/60000]\n",
            "loss: 0.710084  [57664/60000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.665373  [   64/60000]\n",
            "loss: 0.766671  [ 6464/60000]\n",
            "loss: 0.533385  [12864/60000]\n",
            "loss: 0.755088  [19264/60000]\n",
            "loss: 0.669583  [25664/60000]\n",
            "loss: 0.648947  [32064/60000]\n",
            "loss: 0.734024  [38464/60000]\n",
            "loss: 0.721411  [44864/60000]\n",
            "loss: 0.728993  [51264/60000]\n",
            "loss: 0.694188  [57664/60000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.642512  [   64/60000]\n",
            "loss: 0.745307  [ 6464/60000]\n",
            "loss: 0.516077  [12864/60000]\n",
            "loss: 0.741129  [19264/60000]\n",
            "loss: 0.656472  [25664/60000]\n",
            "loss: 0.635971  [32064/60000]\n",
            "loss: 0.714928  [38464/60000]\n",
            "loss: 0.710995  [44864/60000]\n",
            "loss: 0.716371  [51264/60000]\n",
            "loss: 0.679515  [57664/60000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.621831  [   64/60000]\n",
            "loss: 0.725535  [ 6464/60000]\n",
            "loss: 0.500445  [12864/60000]\n",
            "loss: 0.728415  [19264/60000]\n",
            "loss: 0.644587  [25664/60000]\n",
            "loss: 0.624678  [32064/60000]\n",
            "loss: 0.696941  [38464/60000]\n",
            "loss: 0.701895  [44864/60000]\n",
            "loss: 0.705131  [51264/60000]\n",
            "loss: 0.665770  [57664/60000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.603162  [   64/60000]\n",
            "loss: 0.707187  [ 6464/60000]\n",
            "loss: 0.486346  [12864/60000]\n",
            "loss: 0.716697  [19264/60000]\n",
            "loss: 0.633840  [25664/60000]\n",
            "loss: 0.614676  [32064/60000]\n",
            "loss: 0.680058  [38464/60000]\n",
            "loss: 0.694022  [44864/60000]\n",
            "loss: 0.695292  [51264/60000]\n",
            "loss: 0.652864  [57664/60000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.586181  [   64/60000]\n",
            "loss: 0.690247  [ 6464/60000]\n",
            "loss: 0.473570  [12864/60000]\n",
            "loss: 0.705887  [19264/60000]\n",
            "loss: 0.624112  [25664/60000]\n",
            "loss: 0.605896  [32064/60000]\n",
            "loss: 0.664240  [38464/60000]\n",
            "loss: 0.687308  [44864/60000]\n",
            "loss: 0.686716  [51264/60000]\n",
            "loss: 0.640654  [57664/60000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.570709  [   64/60000]\n",
            "loss: 0.674611  [ 6464/60000]\n",
            "loss: 0.461900  [12864/60000]\n",
            "loss: 0.695896  [19264/60000]\n",
            "loss: 0.615305  [25664/60000]\n",
            "loss: 0.598138  [32064/60000]\n",
            "loss: 0.649547  [38464/60000]\n",
            "loss: 0.681650  [44864/60000]\n",
            "loss: 0.679217  [51264/60000]\n",
            "loss: 0.629158  [57664/60000]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.556567  [   64/60000]\n",
            "loss: 0.660137  [ 6464/60000]\n",
            "loss: 0.451194  [12864/60000]\n",
            "loss: 0.686615  [19264/60000]\n",
            "loss: 0.607173  [25664/60000]\n",
            "loss: 0.591141  [32064/60000]\n",
            "loss: 0.635877  [38464/60000]\n",
            "loss: 0.676941  [44864/60000]\n",
            "loss: 0.672746  [51264/60000]\n",
            "loss: 0.618247  [57664/60000]\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.543610  [   64/60000]\n",
            "loss: 0.646719  [ 6464/60000]\n",
            "loss: 0.441366  [12864/60000]\n",
            "loss: 0.677959  [19264/60000]\n",
            "loss: 0.599607  [25664/60000]\n",
            "loss: 0.584772  [32064/60000]\n",
            "loss: 0.623189  [38464/60000]\n",
            "loss: 0.673155  [44864/60000]\n",
            "loss: 0.667129  [51264/60000]\n",
            "loss: 0.607788  [57664/60000]\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.531670  [   64/60000]\n",
            "loss: 0.634285  [ 6464/60000]\n",
            "loss: 0.432300  [12864/60000]\n",
            "loss: 0.669804  [19264/60000]\n",
            "loss: 0.592319  [25664/60000]\n",
            "loss: 0.578890  [32064/60000]\n",
            "loss: 0.611463  [38464/60000]\n",
            "loss: 0.670212  [44864/60000]\n",
            "loss: 0.662295  [51264/60000]\n",
            "loss: 0.597709  [57664/60000]\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.520505  [   64/60000]\n",
            "loss: 0.622807  [ 6464/60000]\n",
            "loss: 0.423901  [12864/60000]\n",
            "loss: 0.662098  [19264/60000]\n",
            "loss: 0.585396  [25664/60000]\n",
            "loss: 0.573341  [32064/60000]\n",
            "loss: 0.600548  [38464/60000]\n",
            "loss: 0.668020  [44864/60000]\n",
            "loss: 0.658127  [51264/60000]\n",
            "loss: 0.588000  [57664/60000]\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.509951  [   64/60000]\n",
            "loss: 0.612128  [ 6464/60000]\n",
            "loss: 0.416234  [12864/60000]\n",
            "loss: 0.654791  [19264/60000]\n",
            "loss: 0.578754  [25664/60000]\n",
            "loss: 0.568153  [32064/60000]\n",
            "loss: 0.590440  [38464/60000]\n",
            "loss: 0.666448  [44864/60000]\n",
            "loss: 0.654499  [51264/60000]\n",
            "loss: 0.578615  [57664/60000]\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.500051  [   64/60000]\n",
            "loss: 0.602266  [ 6464/60000]\n",
            "loss: 0.408967  [12864/60000]\n",
            "loss: 0.647748  [19264/60000]\n",
            "loss: 0.572317  [25664/60000]\n",
            "loss: 0.563148  [32064/60000]\n",
            "loss: 0.581127  [38464/60000]\n",
            "loss: 0.665423  [44864/60000]\n",
            "loss: 0.651258  [51264/60000]\n",
            "loss: 0.569553  [57664/60000]\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.490674  [   64/60000]\n",
            "loss: 0.593127  [ 6464/60000]\n",
            "loss: 0.402235  [12864/60000]\n",
            "loss: 0.641022  [19264/60000]\n",
            "loss: 0.566092  [25664/60000]\n",
            "loss: 0.558265  [32064/60000]\n",
            "loss: 0.572518  [38464/60000]\n",
            "loss: 0.664860  [44864/60000]\n",
            "loss: 0.648486  [51264/60000]\n",
            "loss: 0.560778  [57664/60000]\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.481796  [   64/60000]\n",
            "loss: 0.584658  [ 6464/60000]\n",
            "loss: 0.395991  [12864/60000]\n",
            "loss: 0.634575  [19264/60000]\n",
            "loss: 0.560022  [25664/60000]\n",
            "loss: 0.553531  [32064/60000]\n",
            "loss: 0.564623  [38464/60000]\n",
            "loss: 0.664672  [44864/60000]\n",
            "loss: 0.645967  [51264/60000]\n",
            "loss: 0.552234  [57664/60000]\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.473360  [   64/60000]\n",
            "loss: 0.576761  [ 6464/60000]\n",
            "loss: 0.390090  [12864/60000]\n",
            "loss: 0.628399  [19264/60000]\n",
            "loss: 0.554026  [25664/60000]\n",
            "loss: 0.548862  [32064/60000]\n",
            "loss: 0.557310  [38464/60000]\n",
            "loss: 0.664824  [44864/60000]\n",
            "loss: 0.643642  [51264/60000]\n",
            "loss: 0.543973  [57664/60000]\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.465325  [   64/60000]\n",
            "loss: 0.569449  [ 6464/60000]\n",
            "loss: 0.384547  [12864/60000]\n",
            "loss: 0.622449  [19264/60000]\n",
            "loss: 0.548080  [25664/60000]\n",
            "loss: 0.544262  [32064/60000]\n",
            "loss: 0.550497  [38464/60000]\n",
            "loss: 0.665178  [44864/60000]\n",
            "loss: 0.641416  [51264/60000]\n",
            "loss: 0.536010  [57664/60000]\n",
            "completed training in ... 342.59261560440063s\n",
            "starting timer for testing using CPU...\n",
            "Test Error: \n",
            " Accuracy: 81.0%, Avg loss: 0.548585 \n",
            "\n",
            "completed testing in ... 1.7353622913360596s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Define linear model we will use below\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# modified from https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
        "# get our competing devices ready ... go ahead and init all three here, but ONLY USE ONE during each test\n",
        "# scroll down below and replace the references to gpu_, tpu_, cpu with whichever device you are testing\n",
        "# make sure you replace ALL of them\n",
        "gpu_device = torch.device(\"cuda\")\n",
        "tpu_device = torch.device(\"xla\")\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# first hyperparam\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# Show some sample data\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break\n",
        "\n",
        "\n",
        "# sample code for working with m1\n",
        "  # Create a Tensor directly on the mps device\n",
        "  #x = torch.ones(5, device=mps_device)\n",
        "  # Or\n",
        "  #x = torch.ones(5, device=\"mps\")\n",
        "  # Any operation happens on the GPU\n",
        "  #y = x * 2\n",
        "\n",
        "  # Move your model to mps just like any other device\n",
        "model = NeuralNetwork().to(cpu_device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "print(\"starting timer for training using CPU...\")\n",
        "start = time.time()\n",
        "epochs = 30 \n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer, cpu_device)\n",
        "print(f\"completed training in ... {time.time()-start}s\")\n",
        "\n",
        "print(\"starting timer for testing using CPU...\")\n",
        "start = time.time()\n",
        "test(test_dataloader, model, loss_fn, cpu_device)\n",
        "print(f\"completed testing in ... {time.time()-start}s\")"
      ],
      "metadata": {
        "id": "Un67vsp1EvYK",
        "outputId": "45d3e9be-f81e-4262-94f7-a386b07c7d6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "starting timer for training using CPU...\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.300809  [   64/60000]\n",
            "loss: 2.287689  [ 6464/60000]\n",
            "loss: 2.272648  [12864/60000]\n",
            "loss: 2.264005  [19264/60000]\n",
            "loss: 2.251430  [25664/60000]\n",
            "loss: 2.226089  [32064/60000]\n",
            "loss: 2.225852  [38464/60000]\n",
            "loss: 2.193606  [44864/60000]\n",
            "loss: 2.192010  [51264/60000]\n",
            "loss: 2.158163  [57664/60000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.169783  [   64/60000]\n",
            "loss: 2.155784  [ 6464/60000]\n",
            "loss: 2.099838  [12864/60000]\n",
            "loss: 2.115230  [19264/60000]\n",
            "loss: 2.067963  [25664/60000]\n",
            "loss: 2.008293  [32064/60000]\n",
            "loss: 2.030152  [38464/60000]\n",
            "loss: 1.953758  [44864/60000]\n",
            "loss: 1.962990  [51264/60000]\n",
            "loss: 1.880919  [57664/60000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.920117  [   64/60000]\n",
            "loss: 1.886402  [ 6464/60000]\n",
            "loss: 1.769286  [12864/60000]\n",
            "loss: 1.813733  [19264/60000]\n",
            "loss: 1.706238  [25664/60000]\n",
            "loss: 1.654854  [32064/60000]\n",
            "loss: 1.672584  [38464/60000]\n",
            "loss: 1.580222  [44864/60000]\n",
            "loss: 1.606834  [51264/60000]\n",
            "loss: 1.494230  [57664/60000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.584429  [   64/60000]\n",
            "loss: 1.548853  [ 6464/60000]\n",
            "loss: 1.401018  [12864/60000]\n",
            "loss: 1.479419  [19264/60000]\n",
            "loss: 1.358199  [25664/60000]\n",
            "loss: 1.352930  [32064/60000]\n",
            "loss: 1.359358  [38464/60000]\n",
            "loss: 1.296051  [44864/60000]\n",
            "loss: 1.328252  [51264/60000]\n",
            "loss: 1.223523  [57664/60000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.328945  [   64/60000]\n",
            "loss: 1.310483  [ 6464/60000]\n",
            "loss: 1.147145  [12864/60000]\n",
            "loss: 1.261773  [19264/60000]\n",
            "loss: 1.132328  [25664/60000]\n",
            "loss: 1.155615  [32064/60000]\n",
            "loss: 1.166346  [38464/60000]\n",
            "loss: 1.117836  [44864/60000]\n",
            "loss: 1.154460  [51264/60000]\n",
            "loss: 1.064155  [57664/60000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.158437  [   64/60000]\n",
            "loss: 1.159055  [ 6464/60000]\n",
            "loss: 0.980108  [12864/60000]\n",
            "loss: 1.124963  [19264/60000]\n",
            "loss: 0.994483  [25664/60000]\n",
            "loss: 1.022937  [32064/60000]\n",
            "loss: 1.048216  [38464/60000]\n",
            "loss: 1.003526  [44864/60000]\n",
            "loss: 1.041172  [51264/60000]\n",
            "loss: 0.964557  [57664/60000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.039512  [   64/60000]\n",
            "loss: 1.059587  [ 6464/60000]\n",
            "loss: 0.865264  [12864/60000]\n",
            "loss: 1.033589  [19264/60000]\n",
            "loss: 0.907311  [25664/60000]\n",
            "loss: 0.929363  [32064/60000]\n",
            "loss: 0.971917  [38464/60000]\n",
            "loss: 0.928146  [44864/60000]\n",
            "loss: 0.962363  [51264/60000]\n",
            "loss: 0.898267  [57664/60000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.951679  [   64/60000]\n",
            "loss: 0.989885  [ 6464/60000]\n",
            "loss: 0.783200  [12864/60000]\n",
            "loss: 0.968466  [19264/60000]\n",
            "loss: 0.849165  [25664/60000]\n",
            "loss: 0.860948  [32064/60000]\n",
            "loss: 0.918439  [38464/60000]\n",
            "loss: 0.877283  [44864/60000]\n",
            "loss: 0.904976  [51264/60000]\n",
            "loss: 0.850654  [57664/60000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.883828  [   64/60000]\n",
            "loss: 0.937422  [ 6464/60000]\n",
            "loss: 0.722016  [12864/60000]\n",
            "loss: 0.919678  [19264/60000]\n",
            "loss: 0.807756  [25664/60000]\n",
            "loss: 0.809536  [32064/60000]\n",
            "loss: 0.878021  [38464/60000]\n",
            "loss: 0.841555  [44864/60000]\n",
            "loss: 0.861800  [51264/60000]\n",
            "loss: 0.814483  [57664/60000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.829435  [   64/60000]\n",
            "loss: 0.895393  [ 6464/60000]\n",
            "loss: 0.674819  [12864/60000]\n",
            "loss: 0.882042  [19264/60000]\n",
            "loss: 0.776215  [25664/60000]\n",
            "loss: 0.770135  [32064/60000]\n",
            "loss: 0.845312  [38464/60000]\n",
            "loss: 0.814951  [44864/60000]\n",
            "loss: 0.828117  [51264/60000]\n",
            "loss: 0.785700  [57664/60000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.784497  [   64/60000]\n",
            "loss: 0.860119  [ 6464/60000]\n",
            "loss: 0.636760  [12864/60000]\n",
            "loss: 0.852181  [19264/60000]\n",
            "loss: 0.750707  [25664/60000]\n",
            "loss: 0.739237  [32064/60000]\n",
            "loss: 0.817372  [38464/60000]\n",
            "loss: 0.793981  [44864/60000]\n",
            "loss: 0.800799  [51264/60000]\n",
            "loss: 0.761760  [57664/60000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.746249  [   64/60000]\n",
            "loss: 0.829208  [ 6464/60000]\n",
            "loss: 0.605082  [12864/60000]\n",
            "loss: 0.827726  [19264/60000]\n",
            "loss: 0.729090  [25664/60000]\n",
            "loss: 0.714224  [32064/60000]\n",
            "loss: 0.792462  [38464/60000]\n",
            "loss: 0.776435  [44864/60000]\n",
            "loss: 0.777811  [51264/60000]\n",
            "loss: 0.741081  [57664/60000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.712981  [   64/60000]\n",
            "loss: 0.801428  [ 6464/60000]\n",
            "loss: 0.578014  [12864/60000]\n",
            "loss: 0.807018  [19264/60000]\n",
            "loss: 0.710069  [25664/60000]\n",
            "loss: 0.693474  [32064/60000]\n",
            "loss: 0.769706  [38464/60000]\n",
            "loss: 0.761032  [44864/60000]\n",
            "loss: 0.758008  [51264/60000]\n",
            "loss: 0.722895  [57664/60000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.683752  [   64/60000]\n",
            "loss: 0.776232  [ 6464/60000]\n",
            "loss: 0.554296  [12864/60000]\n",
            "loss: 0.789188  [19264/60000]\n",
            "loss: 0.693315  [25664/60000]\n",
            "loss: 0.675940  [32064/60000]\n",
            "loss: 0.748509  [38464/60000]\n",
            "loss: 0.747267  [44864/60000]\n",
            "loss: 0.740689  [51264/60000]\n",
            "loss: 0.706440  [57664/60000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.657614  [   64/60000]\n",
            "loss: 0.753206  [ 6464/60000]\n",
            "loss: 0.533364  [12864/60000]\n",
            "loss: 0.773483  [19264/60000]\n",
            "loss: 0.678296  [25664/60000]\n",
            "loss: 0.660969  [32064/60000]\n",
            "loss: 0.728660  [38464/60000]\n",
            "loss: 0.734675  [44864/60000]\n",
            "loss: 0.725413  [51264/60000]\n",
            "loss: 0.691311  [57664/60000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.634143  [   64/60000]\n",
            "loss: 0.731957  [ 6464/60000]\n",
            "loss: 0.514734  [12864/60000]\n",
            "loss: 0.759400  [19264/60000]\n",
            "loss: 0.664860  [25664/60000]\n",
            "loss: 0.648045  [32064/60000]\n",
            "loss: 0.709935  [38464/60000]\n",
            "loss: 0.723246  [44864/60000]\n",
            "loss: 0.711836  [51264/60000]\n",
            "loss: 0.677262  [57664/60000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.612972  [   64/60000]\n",
            "loss: 0.712340  [ 6464/60000]\n",
            "loss: 0.498040  [12864/60000]\n",
            "loss: 0.746555  [19264/60000]\n",
            "loss: 0.652777  [25664/60000]\n",
            "loss: 0.636802  [32064/60000]\n",
            "loss: 0.692417  [38464/60000]\n",
            "loss: 0.713069  [44864/60000]\n",
            "loss: 0.699857  [51264/60000]\n",
            "loss: 0.664075  [57664/60000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.593884  [   64/60000]\n",
            "loss: 0.694334  [ 6464/60000]\n",
            "loss: 0.483043  [12864/60000]\n",
            "loss: 0.734867  [19264/60000]\n",
            "loss: 0.641855  [25664/60000]\n",
            "loss: 0.627054  [32064/60000]\n",
            "loss: 0.675951  [38464/60000]\n",
            "loss: 0.704142  [44864/60000]\n",
            "loss: 0.689382  [51264/60000]\n",
            "loss: 0.651729  [57664/60000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.576566  [   64/60000]\n",
            "loss: 0.677814  [ 6464/60000]\n",
            "loss: 0.469524  [12864/60000]\n",
            "loss: 0.724079  [19264/60000]\n",
            "loss: 0.632039  [25664/60000]\n",
            "loss: 0.618421  [32064/60000]\n",
            "loss: 0.660531  [38464/60000]\n",
            "loss: 0.696372  [44864/60000]\n",
            "loss: 0.680256  [51264/60000]\n",
            "loss: 0.640034  [57664/60000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.560855  [   64/60000]\n",
            "loss: 0.662642  [ 6464/60000]\n",
            "loss: 0.457391  [12864/60000]\n",
            "loss: 0.714056  [19264/60000]\n",
            "loss: 0.623161  [25664/60000]\n",
            "loss: 0.610757  [32064/60000]\n",
            "loss: 0.646200  [38464/60000]\n",
            "loss: 0.689784  [44864/60000]\n",
            "loss: 0.672429  [51264/60000]\n",
            "loss: 0.628906  [57664/60000]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.546554  [   64/60000]\n",
            "loss: 0.648754  [ 6464/60000]\n",
            "loss: 0.446400  [12864/60000]\n",
            "loss: 0.704689  [19264/60000]\n",
            "loss: 0.615081  [25664/60000]\n",
            "loss: 0.603890  [32064/60000]\n",
            "loss: 0.632856  [38464/60000]\n",
            "loss: 0.684210  [44864/60000]\n",
            "loss: 0.665724  [51264/60000]\n",
            "loss: 0.618283  [57664/60000]\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.533459  [   64/60000]\n",
            "loss: 0.636045  [ 6464/60000]\n",
            "loss: 0.436331  [12864/60000]\n",
            "loss: 0.695890  [19264/60000]\n",
            "loss: 0.607580  [25664/60000]\n",
            "loss: 0.597600  [32064/60000]\n",
            "loss: 0.620472  [38464/60000]\n",
            "loss: 0.679661  [44864/60000]\n",
            "loss: 0.660099  [51264/60000]\n",
            "loss: 0.608170  [57664/60000]\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.521353  [   64/60000]\n",
            "loss: 0.624378  [ 6464/60000]\n",
            "loss: 0.427105  [12864/60000]\n",
            "loss: 0.687629  [19264/60000]\n",
            "loss: 0.600496  [25664/60000]\n",
            "loss: 0.591839  [32064/60000]\n",
            "loss: 0.608937  [38464/60000]\n",
            "loss: 0.675963  [44864/60000]\n",
            "loss: 0.655357  [51264/60000]\n",
            "loss: 0.598389  [57664/60000]\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.510032  [   64/60000]\n",
            "loss: 0.613566  [ 6464/60000]\n",
            "loss: 0.418671  [12864/60000]\n",
            "loss: 0.679826  [19264/60000]\n",
            "loss: 0.593688  [25664/60000]\n",
            "loss: 0.586440  [32064/60000]\n",
            "loss: 0.598224  [38464/60000]\n",
            "loss: 0.673124  [44864/60000]\n",
            "loss: 0.651354  [51264/60000]\n",
            "loss: 0.588928  [57664/60000]\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.499445  [   64/60000]\n",
            "loss: 0.603555  [ 6464/60000]\n",
            "loss: 0.410891  [12864/60000]\n",
            "loss: 0.672455  [19264/60000]\n",
            "loss: 0.587184  [25664/60000]\n",
            "loss: 0.581325  [32064/60000]\n",
            "loss: 0.588292  [38464/60000]\n",
            "loss: 0.671023  [44864/60000]\n",
            "loss: 0.647889  [51264/60000]\n",
            "loss: 0.579709  [57664/60000]\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.489543  [   64/60000]\n",
            "loss: 0.594276  [ 6464/60000]\n",
            "loss: 0.403725  [12864/60000]\n",
            "loss: 0.665444  [19264/60000]\n",
            "loss: 0.580899  [25664/60000]\n",
            "loss: 0.576348  [32064/60000]\n",
            "loss: 0.579046  [38464/60000]\n",
            "loss: 0.669593  [44864/60000]\n",
            "loss: 0.644899  [51264/60000]\n",
            "loss: 0.570755  [57664/60000]\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.480215  [   64/60000]\n",
            "loss: 0.585669  [ 6464/60000]\n",
            "loss: 0.397058  [12864/60000]\n",
            "loss: 0.658740  [19264/60000]\n",
            "loss: 0.574688  [25664/60000]\n",
            "loss: 0.571433  [32064/60000]\n",
            "loss: 0.570516  [38464/60000]\n",
            "loss: 0.668761  [44864/60000]\n",
            "loss: 0.642302  [51264/60000]\n",
            "loss: 0.561915  [57664/60000]\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.471403  [   64/60000]\n",
            "loss: 0.577709  [ 6464/60000]\n",
            "loss: 0.390856  [12864/60000]\n",
            "loss: 0.652273  [19264/60000]\n",
            "loss: 0.568627  [25664/60000]\n",
            "loss: 0.566530  [32064/60000]\n",
            "loss: 0.562602  [38464/60000]\n",
            "loss: 0.668411  [44864/60000]\n",
            "loss: 0.640125  [51264/60000]\n",
            "loss: 0.553304  [57664/60000]\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.463050  [   64/60000]\n",
            "loss: 0.570334  [ 6464/60000]\n",
            "loss: 0.385053  [12864/60000]\n",
            "loss: 0.646115  [19264/60000]\n",
            "loss: 0.562694  [25664/60000]\n",
            "loss: 0.561712  [32064/60000]\n",
            "loss: 0.555298  [38464/60000]\n",
            "loss: 0.668462  [44864/60000]\n",
            "loss: 0.638245  [51264/60000]\n",
            "loss: 0.545002  [57664/60000]\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.455042  [   64/60000]\n",
            "loss: 0.563488  [ 6464/60000]\n",
            "loss: 0.379671  [12864/60000]\n",
            "loss: 0.640228  [19264/60000]\n",
            "loss: 0.556932  [25664/60000]\n",
            "loss: 0.556903  [32064/60000]\n",
            "loss: 0.548486  [38464/60000]\n",
            "loss: 0.668792  [44864/60000]\n",
            "loss: 0.636520  [51264/60000]\n",
            "loss: 0.536981  [57664/60000]\n",
            "completed training in ... 483.8812458515167s\n",
            "starting timer for testing using CPU...\n",
            "Test Error: \n",
            " Accuracy: 81.1%, Avg loss: 0.545113 \n",
            "\n",
            "completed testing in ... 1.8733329772949219s\n"
          ]
        }
      ]
    }
  ]
}